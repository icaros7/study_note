# 2주차
## 2.1. DBMS
Data Base Management System의 약자

이용자 편의를 위한 데이터베이스 구축 및 유지 관리 소프트웨어    

## 2.2. BigData 5V

|5V|한국어|설명|
|:---:|:---:|:---|
|Volume|크기|네트워크 속도 향상에 따른 수PB 데이터/일 생성
|Variety|다양성|정형(DB), 반정형(XML,HTML), 비정형(Video, Photo)등 데이터 형태의 다양성
|Velocity|속도|정보 유통 및 데이터 가공 및 처리 속도
|Value|가치|유의미한 가치를 가진 지표
|Veracity|정확성|빅데이터를 이용하여 뽑아낸 데이터의 신뢰성

## 2.3. 빅데이터 출현 배경
- 모바일 기기의 확산 및 SNS 활성화가 대표적
- IoT 발달로 데이터 증가 예상
- 이에 따른 선대책

## 2.4. 빅데이터 시대의 변화
|기존|빅데이터|설명|
|:---:|:---:|:---|
|사전처리|사후처리|가공에 따른 리소스 사용보단 많은 양의 데이터 수집 이후 재조합|
|표본조사|전수조사|데이터 처리 비용 감소에 따른 표본 양 증가|
|질|양|양이 증가 할 수록 결과 산출에 긍정적|
|인과관계|상관관계|인과를 찾기보단 상관관계를 통해 특정 현상 발생 가능성 분석 및 도출|

# 4주차
## 4.1. Web Crawling
웹에서 링크를 타고 다니며 직접적으로 웹 페이지를 수집

대표적 예 : Google Bot
## 4.2. Web Scraping
웹에서 정보를 단순 추출만 하는 행위. 상품 이름, 가격등을 추출

대표적 예 : 네이버 쇼핑, 다나와 등

## 4.3. 이통통신 기술 "초저지연"
데이터 통신 속도, 광대역 보다 중요한 것은 초 저지연. 지연이 낮아야 실질적 빅데이터 활용이 가능

## 4.4. 저장 방식
1. 전처리(가공) 후 저장
    - 데이터를 가공을 먼저 한 뒤에 데이터 스토리지에 저장
2. 저장 후 사용시 전처리(가공)
    - 일딴 모든 raw 데이터를 저장 한 뒤 나중에 사용시 가공

## 4.5. 데이터베이스 관리 시스템
- RDBMS의 경우 정형화된 규격에 모든 raw 데이터를 끼워 넣기가 어려움
    - OracleSQL, MSQL, MySQL 등
- NoSQL 등의 새로운 관리 시스템
    - Document, Key-Value, Wide-Column, Graph 방식 등 여러 방식 지원

## 4.6. Data Warehouse
### 4.6.1. 의의
말 그대로 저장창고에 물건을 두기 위해선 각자 관리 체계에 맞게 넣어두듯, raw 데이터를 공통의 형식으로 최대한 원본을 유지하며 형변환을 하여 관리하는 데이터 베이스

### 4.6.2. BI
데이터웨어하우스의 경우 늘 Business Intelligence 라 불리는 BI에 필수적으로 동반되는데, 이는 쉽게 말해

> 사용자(경영진)에게 주요 경영 의사 결정을 쉽게 하기 위함

을 목적으로 데이터 웨어하우스를 사용하기 때문

### 4.6.3. 특징
|특징|설명|
|:---:|:---|
|주제중심성|업무자가 아닌 최종 사용자 기준 맞춤
|통합성|혼재한 DB를 통합 가능|
|시계열성|실시간 저장이 아닌 업무 시간대 별 데이터 보유
|비휘발성|대게 R/O 속성 부여|

### 4.6.4. 구성
대표적인 구성 방식으로는 ETL(Extraction, Transformation and Load), CDC (Change Data Capture)가 존재

1. 데이터 소스 (실질적 정보)
2. 추출 (Extract)
3. 통합 저장 가능한 형식으로 형변환 (Transform)
4. Table에 적재 (Load)
5. 다차원 모델 최적화 (Dimensional Model Optimizing)

# 5주차
## 5.1. 하둡 에코시스템 분석 모형
|모형 명|설명|
|---|---|
|현황분석|과거로부터 이어온 현황 데이터 분석|
|예측분석|지난 데이터를 통한 미래 예측|
|예측최적화|예측 정밀성 증가 및 추가 데이터 적용|

## 5.2. 빅데이터 분석 알고리즘
- 네트워크 분석 알고리즘
- 통계 기반 분석 알고리즘
- 머신러닝 기반 알고리즘
    - 지도 학습
    - 비지도 학습
    - 강화 학습

## 5.3. 데이터 분석 과정
1. 분석 영역 이해
2. 문제점 및 개선점 도출
3. 분석 방법 정의 및 분석
4. 결과 도출

## 5.4. 하둡 마켓 시장
Apache Project Nutch 개선을 통해 Project Hadoop 탄생 (Apache 기반)

|Google Platform API|Hadoop Apache Project|
|---|---|
|File System|Distributed File System|
|MapReduce|MapReduce|

## 5.5. 하둡 생태계 진화
1. 시장 초기엔 큰 데이터를 처리하기 위해선 큰 저장 및 분석 비용이 소요 (고성능 컴퓨터 1대 사용)
2. 하둡 시스템으로 인하여 데이터 저장 및 분석 비용 감소 (저성능 컴퓨터 다수 사용)
3. 클라우드 컴퓨팅 환경 구축으로 인해 컴퓨터 구성 비용 대폭 절감

하둡과 같은 빅데이터 플렛폼 `기술` 발전 -> 데이터 처리 인프라 `가격` 경쟁력 확보 -> 데이터 분석 역량 향상 및 신규 창출 `기회` 생성

## 5.6 Data Scientist
|`데이터`에서|`패턴`을 찾아|`비즈니스`기회로|
|---|---|---|
|프로그래머|통계학자|컨설턴트|

Data analysis requires **imaginative power and tacit knowledges** that are more important than `techniques`.

위 3개의 과정을 한번에 처리할 수 있는 새로운 직업 혹은 구성 팀
## 5.7. Google Platform 과 Hadoop 의 철학
### Google
- 한대의 고가 장비 < 여러대의 저가 장비
- 데이터는 **분산 저장**
- 백업 시스템(H/W) 필수
- 시스템(H/W) 분산 및 자동화의 확장은 무척 간결 해야함

### Hadoop
- 수천대 이상의 리눅스 기반 서버를 **하나의 클러스터**로
- Master - Slave 구조 구성
- 파일은 Block 단위로 구성 저장 (분산 저장)
- 블록 데이터 시스템 별 복사본 유지를 통한 **무결성 보장** (최소 3개)
- 높은 내고장성
- 데이터 처리의 지역성보장 (한 시스템 무결성 검사 실패시 다른 시스템 파일 사용)

# 6주차
## 6.1 데이터 전처리
> 빅데이터 분석 단계 전, 데이터를 전처리 (Preprocessing) 과정 필요. 이는 `데이터 정제` 및 `분석 변수 처리` 과정.

전체 과정의 60~70%를 차지할 만 큼 `노가다` 작업. 이가 잘 된 경우 분석 결과 신뢰성도 증가. 잘 안되는 경우 그 신뢰성도 기하 급수적 감소.

1. 데이터 셋 인지 및 확인
2. 결측값 처리 및 이상값 분류
    - 특정 단일 값 삭제 : 관측치 자체만 삭제, 같은 목록 `다른 변수`는 유지
    - 목록 삭제 : 결측이 발생한 모든 행 또는 열 삭제 (`많은 정보 손실` 발생)
    - 값 대체
        - 특정 값 대체 : 최빈 값, 평균 값, 회귀 추정 값등으로 대표값으로 결측값 Replace
        - 다중 대체 : 주로 복잡한 결측값을 여러번의 추정을 기반으로 Replace
    - 이상 값 검출
3. 다차원 축소 작업
4. 변수 선택 및 파생변수 생성
5. 불균형 데이터 처리